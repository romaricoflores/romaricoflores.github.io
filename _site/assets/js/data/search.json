[
  
  {
    "title": "Taxi Geospatial Data Analysis",
    "url": "/posts/GeospatialAnalysis/",
    "categories": "PORTFOLIO, DATA ANALYSIS",
    "tags": "geospatial analysis, python, folium",
    "date": "2023-08-17 04:00:00 -0700",
    





    
    "snippet": "OverviewThis project delves into geospatial data analysis using a dataset containing information about taxis operating in Proto, Portugal. The aim is to provide a hands-on experience in processing ...",
    "content": "OverviewThis project delves into geospatial data analysis using a dataset containing information about taxis operating in Proto, Portugal. The aim is to provide a hands-on experience in processing and interpreting geospatial data, employing essential visualization techniques, with a focus on using the Folium module in Python.Objectives  Data Understanding and Cleaning: Learn how to prepare geospatial data by cleaning and organizing the taxi dataset, ensuring it’s ready for analysis.  Geospatial Visualization: Gain practical skills in using the Folium module to create maps that showcase taxi routes, pick-up/drop-off points, and patterns in movement.  Hotspot Identification: Identify key areas of high taxi activity to uncover popular pick-up and drop-off locations.  Temporal Analysis: Explore time-based trends in taxi demand and movement to uncover insights about peak hours and lull periods.  Simple Spatial Clustering: Utilize basic clustering techniques to group taxi locations and visualize spatial patterns.import pandas as pd import datetime , calendarimport foliumImporting our datadata = pd.read_csv(\"dataset.csv\")data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH                  0      1372636858620000589      C      1372636858      [(41.141412, -8.618643), (41.141376, -8.618499...              1      1372637303620000596      B      1372637303      [(41.159826, -8.639847), (41.159871, -8.640351...              2      1372636951620000320      C      1372636951      [(41.140359, -8.612964), (41.14035, -8.613378)...              3      1372636854620000520      C      1372636854      [(41.151951, -8.574678), (41.151942, -8.574705...              4      1372637091620000337      C      1372637091      [(41.18049, -8.645994), (41.180517, -8.645949)...      data.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 10000 entries, 0 to 9999Data columns (total 4 columns): #   Column     Non-Null Count  Dtype ---  ------     --------------  -----  0   TRIP_ID    10000 non-null  int64  1   CALL_TYPE  10000 non-null  object 2   TIMESTAMP  10000 non-null  int64  3   TRIP_PATH  10000 non-null  objectdtypes: int64(2), object(2)memory usage: 312.6+ KBColumns  CALL_TYPE: The way used to demand taxi service          ‘A’ : if this trip was dispatched from the central      ‘B’ : if this trip was demanded directly to a taxi driver on a specific stand      ‘C’ : other (i.e. a trip demanded on a random street)            TIMESTAMP: When the trip starts    TRIP_PATH: Contains a list of coordinates          The first element is the coordinates of the trip’s starting point      The last element is the coordinates of the trip’s end point      Data PreprocessingLet’s extract starting and ending points of each tripdata.TRIP_PATH.iloc[0][(41.141412, -8.618643), (41.141376, -8.618499), (41.14251, -8.620326), (41.143815, -8.622153), (41.144373, -8.623953), (41.144778, -8.62668), (41.144697, -8.627373), (41.14521, -8.630226), (41.14692, -8.632746), (41.148225, -8.631738), (41.150385, -8.629938), (41.151213, -8.62911), (41.15124, -8.629128), (41.152203, -8.628786), (41.152374, -8.628687), (41.152518, -8.628759), (41.15268, -8.630838), (41.153022, -8.632323), (41.154489, -8.631144), (41.154507, -8.630829), (41.154516, -8.630829), (41.154498, -8.630829), (41.154489, -8.630838)]See that the cell acts as a string instead of a list, therefore you cant access individual records.# Converts our data to a python list, iloc = integer based indexingdata.TRIP_PATH = data.TRIP_PATH.apply(eval)data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH                  0      1372636858620000589      C      1372636858      [(41.141412, -8.618643), (41.141376, -8.618499...              1      1372637303620000596      B      1372637303      [(41.159826, -8.639847), (41.159871, -8.640351...              2      1372636951620000320      C      1372636951      [(41.140359, -8.612964), (41.14035, -8.613378)...              3      1372636854620000520      C      1372636854      [(41.151951, -8.574678), (41.151942, -8.574705...              4      1372637091620000337      C      1372637091      [(41.18049, -8.645994), (41.180517, -8.645949)...      data.TRIP_PATH.iloc[0][(41.141412, -8.618643), (41.141376, -8.618499), (41.14251, -8.620326), (41.143815, -8.622153), (41.144373, -8.623953), (41.144778, -8.62668), (41.144697, -8.627373), (41.14521, -8.630226), (41.14692, -8.632746), (41.148225, -8.631738), (41.150385, -8.629938), (41.151213, -8.62911), (41.15124, -8.629128), (41.152203, -8.628786), (41.152374, -8.628687), (41.152518, -8.628759), (41.15268, -8.630838), (41.153022, -8.632323), (41.154489, -8.631144), (41.154507, -8.630829), (41.154516, -8.630829), (41.154498, -8.630829), (41.154489, -8.630838)]data.TRIP_PATH.iloc[0][0](41.141412, -8.618643)Extracting the starting point and endpointsgetStart = lambda list_ : list_[0]getEnd = lambda list_ : list_[-1]data[\"START\"] = data.TRIP_PATH.apply(getStart)data[\"END\"] = data.TRIP_PATH.apply(getEnd)data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH      START      END                  0      1372636858620000589      C      1372636858      [(41.141412, -8.618643), (41.141376, -8.618499...      (41.141412, -8.618643)      (41.154489, -8.630838)              1      1372637303620000596      B      1372637303      [(41.159826, -8.639847), (41.159871, -8.640351...      (41.159826, -8.639847)      (41.170671, -8.66574)              2      1372636951620000320      C      1372636951      [(41.140359, -8.612964), (41.14035, -8.613378)...      (41.140359, -8.612964)      (41.14053, -8.61597)              3      1372636854620000520      C      1372636854      [(41.151951, -8.574678), (41.151942, -8.574705...      (41.151951, -8.574678)      (41.142915, -8.607996)              4      1372637091620000337      C      1372637091      [(41.18049, -8.645994), (41.180517, -8.645949)...      (41.18049, -8.645994)      (41.178087, -8.687268)      Remapping CALL_TYPE column values to the proper valuesCALL_TYPES = {              \"A\":\"CENTRAL_BASED\",              \"B\":\"STAND_BASED\",              \"C\":\"OTHER\"             }data.CALL_TYPE = data.CALL_TYPE.map(CALL_TYPES)data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH      START      END                  0      1372636858620000589      OTHER      1372636858      [(41.141412, -8.618643), (41.141376, -8.618499...      (41.141412, -8.618643)      (41.154489, -8.630838)              1      1372637303620000596      STAND_BASED      1372637303      [(41.159826, -8.639847), (41.159871, -8.640351...      (41.159826, -8.639847)      (41.170671, -8.66574)              2      1372636951620000320      OTHER      1372636951      [(41.140359, -8.612964), (41.14035, -8.613378)...      (41.140359, -8.612964)      (41.14053, -8.61597)              3      1372636854620000520      OTHER      1372636854      [(41.151951, -8.574678), (41.151942, -8.574705...      (41.151951, -8.574678)      (41.142915, -8.607996)              4      1372637091620000337      OTHER      1372637091      [(41.18049, -8.645994), (41.180517, -8.645949)...      (41.18049, -8.645994)      (41.178087, -8.687268)      Data AnalysisLet’s answer to some analytical questions using our dataQuestion 1 - What are the most common ways to get a taxi in Porto?q1_data = data.CALL_TYPE.value_counts()q1_data.plot.bar()&lt;Axes: &gt;ImgurQuestion 2 - Which regions of the city are the best pick up points?To answer to this question we can use a dotmap. A dotmap is a scatter plot which is put on a map to represent a geographic data distribution. And then by looking at the different clusters of the points on the map we can get the answer.q1map = folium.Map(location=[41.1579, -8.6291], zoom_start = 12)for point in data.START:    folium.CircleMarker(location=point,    color='red',    radius=1,    weight=2).add_to(Porto)q1mapMake this Notebook Trusted to load map: File -&gt; Trust NotebookQuestion 3 - Which regions of the city are the best pick up points for stand-based trips?To answer to this question we create a dotmap with different Marker clusters, each cluster corresponds to a specific CALL_TYPE.q3map = folium.Map(location=[41.1579, -8.6291], zoom_start = 12)colors = {    \"OTHER\" : \"#003366\", # Dark Blue    \"STAND_BASED\" : \"#66CCFF\", # Light Blue    \"CENTRAL_BASED\" : \"#FF6600\" # Dark Orange}q3data = data[[\"CALL_TYPE\",\"START\"]]for index, row in q3data.iterrows():    color = colors[row[\"CALL_TYPE\"]]    location = row[\"START\"]    folium.CircleMarker(location, color = color, radius=1, weight=2).add_to(q3map)# Adding Legendlegend_html = \"\"\"    &lt;div style=\"position: fixed;                 bottom: 50px; left: 50px; width: 150px; height: 120px;                 background-color: rgba(255, 255, 255, 0.8);                border-radius: 5px; z-index:1000; font-size:14px;                \"&gt;    &amp;nbsp; Legend: &lt;br&gt;    &amp;nbsp; &lt;i class=\"fa fa-circle fa-lg\" style=\"color:#003366\"&gt;&lt;/i&gt; Other &lt;br&gt;    &amp;nbsp; &lt;i class=\"fa fa-circle fa-lg\" style=\"color:#66CCFF\"&gt;&lt;/i&gt; Stand Based &lt;br&gt;    &amp;nbsp; &lt;i class=\"fa fa-circle fa-lg\" style=\"color:#FF6600\"&gt;&lt;/i&gt; Central Based &lt;br&gt;    &lt;/div&gt;    \"\"\"q3map.get_root().html.add_child(folium.Element(legend_html))q3mapMake this Notebook Trusted to load map: File -&gt; Trust NotebookQuestion 4 - Which regions of the city are the most common destinations on Mondays?To answer to this question we need to use the TIMESTAMP column to extract the day each trip took place. day_names = list(calendar.day_name)getDay = lambda timestamp : day_names[datetime.datetime.fromtimestamp(timestamp).weekday()]data[\"WEEK_DAY\"] = data.TIMESTAMP.apply(getDay)data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH      START      END      WEEK_DAY                  0      1372636858620000589      OTHER      1372636858      [(41.141412, -8.618643), (41.141376, -8.618499...      (41.141412, -8.618643)      (41.154489, -8.630838)      Sunday              1      1372637303620000596      STAND_BASED      1372637303      [(41.159826, -8.639847), (41.159871, -8.640351...      (41.159826, -8.639847)      (41.170671, -8.66574)      Sunday              2      1372636951620000320      OTHER      1372636951      [(41.140359, -8.612964), (41.14035, -8.613378)...      (41.140359, -8.612964)      (41.14053, -8.61597)      Sunday              3      1372636854620000520      OTHER      1372636854      [(41.151951, -8.574678), (41.151942, -8.574705...      (41.151951, -8.574678)      (41.142915, -8.607996)      Sunday              4      1372637091620000337      OTHER      1372637091      [(41.18049, -8.645994), (41.180517, -8.645949)...      (41.18049, -8.645994)      (41.178087, -8.687268)      Sunday      Let’s get all the trips on Mondaysq4data = data[data.WEEK_DAY == \"Monday\"]q4data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH      START      END      WEEK_DAY                  443      1372662532620000051      STAND_BASED      1372662532      [(41.140692, -8.615601), (41.140854, -8.615349...      (41.140692, -8.615601)      (41.147145, -8.615223)      Monday              450      1372662153620000351      OTHER      1372662153      [(41.236965, -8.670015), (41.235633, -8.669727...      (41.236965, -8.670015)      (41.16888, -8.590365)      Monday              454      1372663182620000247      CENTRAL_BASED      1372663182      [(41.157639, -8.593767), (41.156982, -8.594208...      (41.157639, -8.593767)      (41.147154, -8.617599)      Monday              464      1372662071620000662      OTHER      1372662071      [(41.156559, -8.641611), (41.156541, -8.641431...      (41.156559, -8.641611)      (41.155875, -8.620776)      Monday              465      1372662631620000621      CENTRAL_BASED      1372662631      [(41.159079, -8.611902), (41.159061, -8.611911...      (41.159079, -8.611902)      (41.149764, -8.619696)      Monday      Now Let’s visualize the data on the mapq4map = folium.Map(location=[41.1579, -8.6291], zoom_start = 12)for loc in q4data.END:        folium.CircleMarker(loc, color = \"red\", radius=1, weight=2).add_to(q4map)q4mapMake this Notebook Trusted to load map: File -&gt; Trust NotebookQuestion 5 - What are the most common start and end points on Monday Mornings (from 6 am to 9 am)?To answer to this question we need to extract the hour information for each of the trips. Let’s extract the hour column from TIMESTAMP columngetHour = lambda timestamp : datetime.datetime.fromtimestamp(timestamp).hourdata[\"HOUR\"] = data.TIMESTAMP.apply(getHour)data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH      START      END      WEEK_DAY      HOUR                  0      1372636858620000589      OTHER      1372636858      [(41.141412, -8.618643), (41.141376, -8.618499...      (41.141412, -8.618643)      (41.154489, -8.630838)      Sunday      17              1      1372637303620000596      STAND_BASED      1372637303      [(41.159826, -8.639847), (41.159871, -8.640351...      (41.159826, -8.639847)      (41.170671, -8.66574)      Sunday      17              2      1372636951620000320      OTHER      1372636951      [(41.140359, -8.612964), (41.14035, -8.613378)...      (41.140359, -8.612964)      (41.14053, -8.61597)      Sunday      17              3      1372636854620000520      OTHER      1372636854      [(41.151951, -8.574678), (41.151942, -8.574705...      (41.151951, -8.574678)      (41.142915, -8.607996)      Sunday      17              4      1372637091620000337      OTHER      1372637091      [(41.18049, -8.645994), (41.180517, -8.645949)...      (41.18049, -8.645994)      (41.178087, -8.687268)      Sunday      17      q5data = data[(data.WEEK_DAY == \"Monday\") &amp; (data.HOUR &gt; 6) &amp; (data.HOUR &lt; 9)]q5data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH      START      END      WEEK_DAY      HOUR                  2376      1372687382620000167      STAND_BASED      1372687382      [(41.147766, -8.6202), (41.147442, -8.621181),...      (41.147766, -8.6202)      (41.152464, -8.627031)      Monday      7              2414      1372687571620000562      STAND_BASED      1372687571      [(41.145687, -8.610858), (41.145768, -8.610876...      (41.145687, -8.610858)      (41.150781, -8.622954)      Monday      7              2422      1372687466620000372      STAND_BASED      1372687466      [(41.154498, -8.649315), (41.154237, -8.649747...      (41.154498, -8.649315)      (41.173848, -8.688843)      Monday      7              2425      1372687350620000408      CENTRAL_BASED      1372687350      [(41.151573, -8.566362), (41.151555, -8.566335...      (41.151573, -8.566362)      (41.151816, -8.609661)      Monday      7              2434      1372687233620000426      STAND_BASED      1372687233      [(41.148621, -8.585856), (41.148909, -8.585811...      (41.148621, -8.585856)      (41.154561, -8.649477)      Monday      7      Now let’s visualize this dataq5map = folium.Map(location=[41.15, -8.62], zoom_start=12)AddStart = lambda loc : folium.CircleMarker(loc, color = \"red\", radius=1, weight=2).add_to(q5map)AddEnd = lambda loc : folium.CircleMarker(loc, color = \"blue\", radius=1, weight=2).add_to(q5map)q5data.START.apply(AddStart)q5data.END.apply(AddEnd)q5mapMake this Notebook Trusted to load map: File -&gt; Trust NotebookQuestion 6 - Find out the rush hour in Portodata.HOUR.value_counts().sort_index().plot.bar()&lt;Axes: &gt;Question 7 - Which streets has the more traffic during the rush hourTo answer to this question we need to use TRIP_PATH column to visualize each path as a line on the map. Firstly, let's filter out the needed data. q7data = data[data.HOUR == 2]q7data.head()                  TRIP_ID      CALL_TYPE      TIMESTAMP      TRIP_PATH      START      END      WEEK_DAY      HOUR                  937      1372669784620000455      OTHER      1372669784      [(41.166252, -8.607528), (41.165748, -8.608149...      (41.166252, -8.607528)      (41.164875, -8.608536)      Monday      2              947      1372669436620000285      STAND_BASED      1372669436      [(41.145606, -8.610813), (41.145777, -8.610813...      (41.145606, -8.610813)      (41.149242, -8.627157)      Monday      2              956      1372669271620000653      STAND_BASED      1372669271      [(41.15772, -8.628399)]      (41.15772, -8.628399)      (41.15772, -8.628399)      Monday      2              965      1372669911620000455      OTHER      1372669911      [(41.164857, -8.608527), (41.16474, -8.608509)...      (41.164857, -8.608527)      (41.155362, -8.610237)      Monday      2              972      1372669864620000050      CENTRAL_BASED      1372669864      [(41.155866, -8.643798), (41.156811, -8.643591...      (41.155866, -8.643798)      (41.148882, -8.648964)      Monday      2      q7map = folium.Map(location=[41.15,-8.62], zoom_start=12)for coords in q7data.TRIP_PATH:    folium.PolyLine(coords, color = \"red\", opacity=0.1, weight=2).add_to(q7map)q7mapMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  
  {
    "title": "JobMatch App Project",
    "url": "/posts/StreamlitJobMatch/",
    "categories": "PORTFOLIO, MACHINE LEARNING",
    "tags": "streamlit, nlp, machine learning",
    "date": "2023-08-14 06:00:00 -0700",
    





    
    "snippet": "This project is a tool to assess how well your resume matches a job posting. Simply input your resume and the job description, and let the app calculate the match score. The project uses Spacy whic...",
    "content": "This project is a tool to assess how well your resume matches a job posting. Simply input your resume and the job description, and let the app calculate the match score. The project uses Spacy which is a powerful and efficient open-source library for working with text and performing tasks like understanding the meaning of words, identifying named entities (like names of people or places), and analyzing sentence structure.The process begins as Streamlit captures your resume and the job posting. Utilizing Spacy, it tokenizes and preprocess the textual content, creating linguistic representations. These representations are then evaluated for semantic similarity, yielding a match score that quantifies alignment.The match score, ranging from 0 to 100%, shows the convergence of your skills and the job requirements. The results are also interpreted as a ‘Strong Match,’ ‘Moderate Match,’ ‘Weak Match,’ or ‘Low Match.’ based on the skills and experience required for the position."
  },
  
  {
    "title": "Digital Elevation 3D Modelling in ArcGIS Pro Global Scene Editor",
    "url": "/posts/Pkols/",
    "categories": "PORTFOLIO, GIS",
    "tags": "arcgis pro, 3d modeling, gis",
    "date": "2023-08-11 10:51:00 -0700",
    





    
    "snippet": "    Mt Pkols by Roma Rico FloresIntroduction:Creating a 3D digital elevation model (DEM) of a real-world location can provide valuable insights and visualizations for various purposes, from urban p...",
    "content": "    Mt Pkols by Roma Rico FloresIntroduction:Creating a 3D digital elevation model (DEM) of a real-world location can provide valuable insights and visualizations for various purposes, from urban planning to environmental analysis. In this tutorial, we will walk you through the process of creating a 3D DEM of Mount Douglas in Saanich, British Columbia, Canada, using the ArcGIS Pro Global Scene Editor. This powerful tool allows you to visualize and analyze geographic data in a three-dimensional space, providing a detailed representation of the terrain.Prerequisites:  ArcGIS Pro installed on your computer  Access to elevation data (Digital Elevation Model or LiDAR data) for Mount Douglas  Basic familiarity with ArcGIS Pro and its interfaceStep 1: Import Elevation Data:  Open ArcGIS Pro and create a new project.  Add your elevation data.Step 2: Create a New Global Scene:  From the “Insert” tab, select “New Map” and choose “Global Scene.”  Set the coordinate system and map extent to match your elevation data.Step 3: Add Elevation Data to the Scene:  In the “Contents” pane, right-click on “Scene Layers” and select “Add Data.”  Choose the elevation data you imported in Step 1 and add it to the global scene.Step 4: Adjust Elevation Surface Settings:  Select the elevation layer in the “Contents” pane.  In the “Appearance” tab, adjust the elevation surface settings such as Minimum and Maximum values to accurately represent the terrain of Mount Douglas.Step 5: Add Imagery:  To enhance the visualization, add imagery by clicking on the “Insert” tab and selecting “New Basemap Layer.” Choose an appropriate basemap for your project.Step 6: Navigate and Explore:  Use the navigation tools (pan, zoom, rotate) in the Global Scene Editor to explore the 3D terrain of Mount Douglas.  Adjust the vertical exaggeration if necessary to emphasize the elevation differences.Step 7: Add Additional Data (Optional):  You can enhance your 3D scene by adding additional data layers, such as roads, water bodies, vegetation, and buildings. This will provide context and depth to your model.Step 8: Customize Appearance (Optional):  Experiment with different rendering and visualization options to highlight specific features or aspects of the terrain.Step 9: Analyze and Interpret:  Utilize the analysis tools in ArcGIS Pro to perform various tasks, such as line of sight analysis, slope analysis, or viewshed analysis, to gain insights into the terrain’s characteristics.Step 10: Save and Share:  Once you’re satisfied with your 3D DEM, save your project.  You can share your project with others by creating a web scene or exporting images and videos of your 3D model.Conclusion:Creating a 3D digital elevation model of Mount Douglas in Saanich, BC using ArcGIS Pro Global Scene Editor allows you to visualize and analyze the terrain in a dynamic and insightful way. By following the steps outlined in this tutorial, you can create a detailed and accurate representation of the landscape, providing valuable information for various applications."
  },
  
  {
    "title": "Getting Started with Earth Engine",
    "url": "/posts/EarthEngine/",
    "categories": "PORTFOLIO, TUTORIALS",
    "tags": "google earth engine, spatial analytics, gis",
    "date": "2023-08-11 10:51:00 -0700",
    





    
    "snippet": "IntroductionGoogle Earth Engine is a powerful platform that combines satellite imagery, geospatial data, and cloud computing to provide users with the ability to analyze and visualize Earth’s chang...",
    "content": "IntroductionGoogle Earth Engine is a powerful platform that combines satellite imagery, geospatial data, and cloud computing to provide users with the ability to analyze and visualize Earth’s changing landscape. Whether you’re an environmental researcher, a conservationist, a developer, or simply someone fascinated by the world around you, having a Google Earth Engine account opens up a world of possibilities. In this guide, we’ll walk you through the process of creating your own Google Earth Engine account so you can start exploring and contributing to this remarkable platform.Step 1: Visit the Google Earth Engine WebsiteTo begin, open your web browser and navigate to the Google Earth Engine website. You can do this by typing “Google Earth Engine” into your search engine or by directly entering “earthengine.google.com” in the address bar.Step 2: Sign In with Your Google AccountSince Google Earth Engine is a Google product, you’ll need a Google account to access it. If you already have a Google account (such as a Gmail account), click on the “Sign In” button at the top right corner of the webpage. Enter your Google account email address and password to proceed.If you don’t have a Google account, you’ll need to create one. Click on the “Create Account” button and follow the prompts to set up your new Google account. Once you’ve created an account, return to the Google Earth Engine website and sign in using your newly created credentials.Step 3: Request AccessAfter signing in, you’ll be directed to the Google Earth Engine main page. Here, you’ll see information about Earth Engine and its capabilities. To gain access to the platform, click on the “Request Access” button. This step is necessary because Google Earth Engine access is not automatically granted to everyone.Step 4: Fill Out the Access Request FormThe access request form will ask you for some basic information about yourself, your organization (if applicable), and your intended use of Google Earth Engine. Be sure to provide accurate and detailed information, as this will help Google evaluate your request. Explain how you plan to use the platform and why you are interested in accessing it. Once you’ve completed the form, click “Submit.”Step 5: Wait for ApprovalAfter submitting your access request, Google will review your application. The approval process may take some time, so be patient. You’ll receive an email notification once your access request has been approved or denied.Step 6: Access Granted!Congratulations! Once your access request is approved, you’ll be notified via email. The email will provide instructions on how to log in to your Google Earth Engine account. Return to the Google Earth Engine website and sign in using your Google account credentials.Step 7: Explore and Utilize Google Earth EngineWith your Google Earth Engine account, you can now explore the vast collection of satellite imagery and geospatial data, analyze environmental changes, create custom visualizations, and even develop your own geospatial applications using the Earth Engine API. Take advantage of the various tutorials, documentation, and resources provided by Google to make the most of your Earth Engine experience.ConclusionCreating a Google Earth Engine account opens up a world of possibilities for analyzing, visualizing, and contributing to our understanding of Earth’s dynamic landscapes. By following the steps outlined in this guide, you can successfully request and obtain access to the platform. With your newfound access, you’ll have the tools you need to embark on exciting geospatial journeys and contribute to the global effort of monitoring and conserving our planet."
  },
  
  {
    "title": "NYC Traffic Collision Interactive Data Analysis",
    "url": "/posts/StreamlitCollision/",
    "categories": "PORTFOLIO, TUTORIALS",
    "tags": "streamlit, plotly, interactive",
    "date": "2023-07-11 10:51:00 -0700",
    





    
    "snippet": "",
    "content": ""
  },
  
  {
    "title": "Vancouver Crime Analysis",
    "url": "/posts/VanCrime/",
    "categories": "PORTFOLIO, DATA ANALYSIS",
    "tags": "crime analysis, data analysis, gis",
    "date": "2023-07-11 10:51:00 -0700",
    





    
    "snippet": "NOTE: Some interactivity features are disabled in this page, to access full interactive visualizations, access and run the notebook in Kaggle.Vancouver Crime Analysisimport pandas as pdimport matpl...",
    "content": "NOTE: Some interactivity features are disabled in this page, to access full interactive visualizations, access and run the notebook in Kaggle.Vancouver Crime Analysisimport pandas as pdimport matplotlib.pyplot as pltimport numpy as npimport plotly.subplots as spimport plotly.graph_objects as goimport seaborn as snsimport ipywidgets as widgetsfrom IPython.display import displayimport geopandas as gpdimport foliumDatasetdf_vancrime = pd.read_csv(\"/kaggle/input/crime-in-vancouver/crime.csv\")df_vancrime.head()                  TYPE      YEAR      MONTH      DAY      HOUR      MINUTE      HUNDRED_BLOCK      NEIGHBOURHOOD      X      Y      Latitude      Longitude                  0      Other Theft      2003      5      12      16.0      15.0      9XX TERMINAL AVE      Strathcona      493906.5      5457452.47      49.269802      -123.083763              1      Other Theft      2003      5      7      15.0      20.0      9XX TERMINAL AVE      Strathcona      493906.5      5457452.47      49.269802      -123.083763              2      Other Theft      2003      4      23      16.0      40.0      9XX TERMINAL AVE      Strathcona      493906.5      5457452.47      49.269802      -123.083763              3      Other Theft      2003      4      20      11.0      15.0      9XX TERMINAL AVE      Strathcona      493906.5      5457452.47      49.269802      -123.083763              4      Other Theft      2003      4      12      17.0      45.0      9XX TERMINAL AVE      Strathcona      493906.5      5457452.47      49.269802      -123.083763      df_vancrime.shape(530652, 12)df_vancrime.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 530652 entries, 0 to 530651Data columns (total 12 columns): #   Column         Non-Null Count   Dtype  ---  ------         --------------   -----   0   TYPE           530652 non-null  object  1   YEAR           530652 non-null  int64   2   MONTH          530652 non-null  int64   3   DAY            530652 non-null  int64   4   HOUR           476290 non-null  float64 5   MINUTE         476290 non-null  float64 6   HUNDRED_BLOCK  530639 non-null  object  7   NEIGHBOURHOOD  474028 non-null  object  8   X              530652 non-null  float64 9   Y              530652 non-null  float64 10  Latitude       530652 non-null  float64 11  Longitude      530652 non-null  float64dtypes: float64(6), int64(3), object(3)memory usage: 48.6+ MB# I'll drop X and Y because I'll be using Lat and Long instead laterdf_vancrime = df_vancrime.drop(['X', 'Y'], axis=1)# Checking unique values in the following columnscolumns = ['YEAR', 'TYPE', 'NEIGHBOURHOOD']# Getting unique values for each columnfor item in columns:    unique_values = df_vancrime[item].unique()    print(f\"Unique values for '{item}':\")    print(unique_values)    print()Unique values for 'YEAR':[2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017]Unique values for 'TYPE':['Other Theft' 'Break and Enter Residential/Other' 'Mischief' 'Break and Enter Commercial' 'Offence Against a Person' 'Theft from Vehicle' 'Vehicle Collision or Pedestrian Struck (with Injury)' 'Vehicle Collision or Pedestrian Struck (with Fatality)' 'Theft of Vehicle' 'Homicide' 'Theft of Bicycle']Unique values for 'NEIGHBOURHOOD':['Strathcona' 'Kerrisdale' 'Dunbar-Southlands' 'Grandview-Woodland' 'Sunset' 'West End' nan 'Central Business District' 'Hastings-Sunrise' 'Victoria-Fraserview' 'Fairview' 'Kensington-Cedar Cottage' 'West Point Grey' 'Shaughnessy' 'Renfrew-Collingwood' 'Killarney' 'Riley Park' 'Arbutus Ridge' 'Musqueam' 'Mount Pleasant' 'Kitsilano' 'Stanley Park' 'South Cambie' 'Marpole' 'Oakridge']Analysisdf_vancrime['TYPE'].value_counts()Theft from Vehicle                                        172700Mischief                                                   70413Break and Enter Residential/Other                          60862Offence Against a Person                                   54142Other Theft                                                52167Theft of Vehicle                                           38418Break and Enter Commercial                                 33845Theft of Bicycle                                           25730Vehicle Collision or Pedestrian Struck (with Injury)       21901Vehicle Collision or Pedestrian Struck (with Fatality)       254Homicide                                                     220Name: TYPE, dtype: int64# Sort the DataFrame by frequency in descending ordersorted_counts = df_vancrime['TYPE'].value_counts(ascending=True)# Create the bar plotplt.figure(figsize=(10, 6))bars = plt.barh(sorted_counts.index, sorted_counts)# Add labels to the barsfor bar in bars:    width = bar.get_width()    plt.text(width + 800, bar.get_y() + bar.get_height()/2, str(width))# Set labels and titleplt.xlabel('Count')plt.ylabel('Crime Type')plt.title('Frequency of Crime Types (Sorted)')# Remove gridlinesplt.grid(False)# Show the plotplt.show()Simplified Crime Type Categories# Combining and simplifying categoriesdf_vancrime['TYPE'] = df_vancrime['TYPE'].replace({    'Vehicle Collision or Pedestrian Struck (with Injury)': 'Collision',    'Vehicle Collision or Pedestrian Struck (with Fatality)': 'Collision'})df_vancrime['TYPE'] = df_vancrime['TYPE'].replace({    'Break and Enter Residential/Other': 'Break-in',    'Break and Enter Commercial': 'Break-in'})df_vancrime['TYPE'] = df_vancrime['TYPE'].replace({    'Theft of Bicycle': 'Theft of Vehicle',})df_vancrime['TYPE'] = df_vancrime['TYPE'].replace({    'Offence Against a Person': 'Offence',})df_vancrime['TYPE'].value_counts()Theft from Vehicle    172700Break-in               94707Mischief               70413Theft of Vehicle       64148Offence                54142Other Theft            52167Collision              22155Homicide                 220Name: TYPE, dtype: int64# Checking NaNs in the datasetnan_counts = df_vancrime.isnull().sum(axis=0)print(nan_counts)TYPE                 0YEAR                 0MONTH                0DAY                  0HOUR             54362MINUTE           54362HUNDRED_BLOCK       13NEIGHBOURHOOD    56624Latitude             0Longitude            0dtype: int64# Define the value countsvalue_counts = df_vancrime['TYPE'].value_counts()# Create the bar chartplt.figure(figsize=(10, 6))sns.barplot(x=value_counts.index, y=value_counts, color='#ADD8E6')# Add value count labels to the barsfor i, v in enumerate(value_counts):    plt.text(i, v, str(v), ha='center', va='bottom')# Customize the chartplt.xlabel('Crime Type')plt.ylabel('Count')plt.title('Vancouver Crime Types')plt.xticks(rotation=45)sns.despine()# Display the chartplt.show()Trend Analysis(Double-click Legend Item to Isolate Time Series)import plotly.express as px# Group the dataframe by 'TYPE' and aggregate by count for each yeartype_counts = df_vancrime.groupby(['TYPE', 'YEAR']).size().reset_index(name='Count')# Create the interactive time series chartfig = px.line(type_counts, x='YEAR', y='Count', color='TYPE',              title='Yearly Time Series of Crime Types')# Update layoutfig.update_layout(width=1000, height=500)# Display the chartfig.show()                                                Crime Type Weekly Time and Day Heatmap(Dropdowns Need be Run to Work)# Function to generate the heatmap based on the selected crime typedef generate_crime_heatmap(crime_type):    # Filter the dataframe for the specific crime type    filtered_df = df_vancrime[df_vancrime['TYPE'] == crime_type].copy()    # Combine 'YEAR', 'MONTH', and 'DAY' columns to create a new 'DATE' column    filtered_df['DATE'] = pd.to_datetime(filtered_df[['YEAR', 'MONTH', 'DAY']])    # Convert the 'DATE' column to day of the week (e.g., Monday, Tuesday, etc.)    filtered_df['DAY_OF_WEEK'] = filtered_df['DATE'].dt.day_name()    # Group the data by hour (in 12-hour format) and day of the week and calculate the count    heatmap_data = filtered_df.groupby(['HOUR', 'DAY_OF_WEEK']).size().reset_index(name='Count')    # Create a pivot table to reshape the data for the heatmap    pivot_table = heatmap_data.pivot(index='HOUR', columns='DAY_OF_WEEK', values='Count')    # Create the heatmap    plt.figure(figsize=(10, 8))  # Set the figure size    heatmap = sns.heatmap(pivot_table, annot=True, cmap=\"Reds\", fmt='d', cbar=False)    # Customize the x-axis labels (days of the week) and y-axis labels (hours in 12-hour format)    plt.xticks(ticks=range(7), labels=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], verticalalignment='center')    plt.yticks(ticks=range(24), labels=['12 AM'] + [f'{h} AM' for h in range(1, 12)] + ['12 PM'] + [f'{h} PM' for h in range(1, 12)], verticalalignment='center')    # Customize the chart    plt.title(f'Weekly {crime_type} Heatmap')    plt.xlabel('Day of the Week')    plt.ylabel('Hour')    # Display the chart    plt.show()# Create a dropdown widget with crime typescrime_dropdown = widgets.Dropdown(    options=[        'Theft from Vehicle',        'Break-in',        'Mischief',        'Theft of Vehicle',        'Other Theft',        'Collision',    ],    description='Crime Type:',    value='Theft from Vehicle')# Create an interactive widget using the dropdown and the generate_crime_heatmap functioninteractive_widget = widgets.interact(generate_crime_heatmap, crime_type=crime_dropdown)# Display the widgetdisplay(interactive_widget)interactive(children=(Dropdown(description='Crime Type:', options=('Theft from Vehicle', 'Break-in', 'Mischief…&lt;function __main__.generate_crime_heatmap(crime_type)&gt;# Crimes by hour of the daysns.catplot(x='HOUR',           kind='count',            height=8.27,             aspect=3,            color='#E55451',           data=df_vancrime)plt.xticks(size=20)plt.yticks(size=20)plt.xlabel('Hour', fontsize=30)plt.ylabel('Count', fontsize=30)Text(-12.805555555555548, 0.5, 'Count')# Crimes by month of yearmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']sns.catplot(x='MONTH',           kind='count',            height=8,             aspect=3,            color='#E55451',           data=df_vancrime)plt.xticks(np.arange(12), months, size=20)plt.yticks(size=20)plt.xlabel('')plt.ylabel('Count', fontsize=30)Text(-12.805555555555548, 0.5, 'Count')Spatial Analysis# Adding neighborhoods boundariesgdf_neighborhoods = gpd.read_file(\"/kaggle/input/vancouver-airbnb-data/neighbourhoods.geojson\")# Drop empty columngdf_neighborhoods = gdf_neighborhoods.drop(\"neighbourhood_group\", axis=1)gdf_neighborhoods.drop_duplicates(subset=['neighbourhood'], keep='first', inplace=True)gdf_neighborhoods.crs = \"EPSG:4326\"gdf_neighborhoods.head()                  neighbourhood      geometry                  0      Dunbar Southlands      MULTIPOLYGON (((-123.17909 49.21708, -123.1791...              1      Renfrew-Collingwood      MULTIPOLYGON (((-123.05888 49.23286, -123.0588...              2      Hastings-Sunrise      MULTIPOLYGON (((-123.05660 49.26215, -123.0566...              3      Oakridge      MULTIPOLYGON (((-123.14015 49.21935, -123.1401...              4      Downtown Eastside      MULTIPOLYGON (((-123.09988 49.27604, -123.1022...      unique_entries = df_vancrime[~df_vancrime['NEIGHBOURHOOD'].isin(gdf_neighborhoods['neighbourhood'])]['NEIGHBOURHOOD'].unique()print(unique_entries)['Dunbar-Southlands' nan 'Central Business District' 'Musqueam' 'Stanley Park']# Convert df_vancrime to a GeoDataFramegdf_vancrime = gpd.GeoDataFrame(df_vancrime, geometry=gpd.points_from_xy(df_vancrime.Longitude, df_vancrime.Latitude))gdf_vancrime.crs = \"EPSG:4326\"# Perform spatial joinjoined_df = gpd.sjoin(gdf_vancrime, gdf_neighborhoods, how='left', predicate='within')# Update the \"NEIGHBOURHOOD\" column in df_vancrime with the values from the joined_dfdf_vancrime['NEIGHBOURHOOD'] = joined_df['neighbourhood']# Rename the columngdf_neighborhoods = gdf_neighborhoods.rename(columns={'neighbourhood': 'NEIGHBOURHOOD'})# Group the crime data by neighborhood and count the number of crimescrime_count = gdf_vancrime.groupby('NEIGHBOURHOOD').size().reset_index(name='crime_count')# Merge the crime counts with the neighborhood polygonsmerged_data = gdf_neighborhoods.merge(crime_count, on='NEIGHBOURHOOD', how='left')Choropleth Mapfrom folium.plugins import HeatMap# Create a map centered on Vancouvervancouver_map = folium.Map(location=[49.26, -123.1207], zoom_start=12., tiles='cartodbpositron', attr='CartoDB')# Disable zooming and scrollingvancouver_map.options.update({    'scrollWheelZoom': False,    'touchZoom': False,    'doubleClickZoom': False,    'boxZoom': False,    'dragging': False})# Create a choropleth map layerfolium.Choropleth(    geo_data=merged_data,    name='Crime Count',    data=merged_data,    columns=['NEIGHBOURHOOD', 'crime_count'],    key_on='feature.properties.NEIGHBOURHOOD',    fill_color='YlOrRd',    fill_opacity=0.7,    line_opacity=0.2,    legend_name='Crime Count').add_to(vancouver_map)# Add neighborhood labels to the mapfor index, row in merged_data.iterrows():    label = folium.map.Marker(        location=[row.geometry.centroid.y, row.geometry.centroid.x],        icon=folium.DivIcon(html=f'&lt;div style=\"font-size: 8pt; font-weight: bold;\"&gt;{row[\"NEIGHBOURHOOD\"]}&lt;/div&gt;')    )    label.add_to(vancouver_map)# Add a layer control panel to the mapfolium.LayerControl().add_to(vancouver_map)# Display the mapvancouver_mapMake this Notebook Trusted to load map: File -&gt; Trust NotebookCluster Map(Dropdowns Need be Run to Work; Zoom to Show Details)from folium.plugins import MarkerClusterimport ipywidgets as widgetsfrom IPython.display import display# Create basic Folium crime mapcrime_map = folium.Map(location=[49.26, -123.1207],                       tiles=\"cartodbpositron\",                       attr='CartoDB',                       zoom_start=12)# Function to update the data points based on the selected yeardef update_data_points(year):    # Clear existing marker cluster layer    crime_map.get_root().html.add_child(folium.Element(\"&lt;script&gt;document.getElementById('marker_cluster').remove();&lt;/script&gt;\"))    # Filter data for the selected year    data_points = gdf_vancrime[gdf_vancrime.YEAR == year]    data_points = data_points[['Latitude', 'Longitude']]    data_points = data_points.dropna(axis=0, subset=['Latitude', 'Longitude'])    # Create and add marker cluster layer to the map    marker_cluster = MarkerCluster(name='Crimes', id='marker_cluster')    for _, row in data_points.iterrows():        marker = folium.Marker(            location=[row['Latitude'], row['Longitude']],            icon=None,        )        marker.add_to(marker_cluster)    marker_cluster.add_to(crime_map)# Dropdown widget for selecting the yearyear_dropdown = widgets.Dropdown(    options=list(range(2003, 2018)),    value=2014,    description='Year:',    disabled=False,)# Event handler for the dropdown widgetdef on_year_change(change):    year = change.new    update_data_points(year)# Register the event handleryear_dropdown.observe(on_year_change, 'value')# Display the dropdown widgetdisplay(year_dropdown)# Add initial data points to the mapupdate_data_points(year_dropdown.value)# Display the mapcrime_mapDropdown(description='Year:', index=11, options=(2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2…Make this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  
  {
    "title": "Create Your Own Geocoder App in Google Sheets",
    "url": "/posts/Geocode/",
    "categories": "PORTFOLIO, TUTORIALS",
    "tags": "gis, geocoding, google sheets",
    "date": "2023-07-11 10:51:00 -0700",
    





    
    "snippet": "Google Apps Script is a powerful way of adding additional functionality to Google Sheets, enabling you to add custom menus and functions, as well as integrations with other Google services. Here, w...",
    "content": "Google Apps Script is a powerful way of adding additional functionality to Google Sheets, enabling you to add custom menus and functions, as well as integrations with other Google services. Here, we’ll add two functions that can call Google Maps service and Yandex Maps Service, and use that service to get the location coordinates. Note that both services are subject to quota.Open up a new Google Spreadsheet and go to the script editor, under the Tools menu and paste in the Apps Script code below:/** * Returns latitude and longitude values for given address using the Google Maps Geocoder. * * @param {string} address - The address you get the latitude and longitude for. * @customfunction */function GEOCODE_GOOGLE(address) {    if (address.map) {        return address.map(GEOCODE_GOOGLE)    } else {        var r = Maps.newGeocoder().geocode(address)        for (var i = 0; i &lt; r.results.length; i++) {            var res = r.results            return res.geometry.location.lat + \", \" + res.geometry.location.lng        }    }}/** * Returns latitude and longitude values for given address using the Yandex Geocoder. * * @param {string} address - The address you get the latitude and longitude for. * @customfunction */function GEOCODE_YANDEX(address) {    if (address.map) {        return address.map(GEOCODE_YANDEX)    } else {        input = encodeURI(address)        var r = UrlFetchApp.fetch(            \"https://geocode-maps.yandex.ru/1.x/?format=json&amp;geocode=\" +            input + \"&amp;results=1&amp;lang=en-US\", {                \"method\": \"get\"            })        var res = JSON.parse(r)        try {            res = res.response.GeoObjectCollection.featureMember[0].GeoObject.Point.pos            res = res.split(\" \")[1] + \", \" + res.split(\" \")[0]            return res        } catch (e) {            return \"\"        }    }}As you can see, we’re defining two functions that takes an address as input. The metadata that is added on top function gives more context on how it can be used. Hit save and return back to the spreadsheet.On the spreadsheet, add the addresses you want to return the geo coordinates for. Both geocoders are quite accurate, but it is still important to be specific about addresses. For example, “Rialto”, would most probably bring you to the famous Rialto Bridge in Venice, Italy, even though you really meant “The Bird Flanagan Pub, Rialto, Dublin”. Try appending a city, state or country name if you only have a short address.I. Creating a custom functionTo write a custom function:  Create or open a spreadsheet in Google Sheets.  Select the menu item Extensions &gt; Apps Script.  Delete any code in the script editor. For the DOUBLE function above, simply copy and paste the code into the script editor.  At the top, click Save save."
  },
  
  {
    "title": "Coffee Business Dashboard Using Tableau",
    "url": "/posts/CoffeeBusiness/",
    "categories": "PORTFOLIO, DATA ANALYSIS, DATA VISUALIZATION, TABLEAU",
    "tags": "data analysis, tableau, dashboards, data visualization",
    "date": "2023-05-26 10:51:00 -0700",
    





    
    "snippet": "  Access Data and Dashbaord in Tableau Public Tableau Public.   ",
    "content": "  Access Data and Dashbaord in Tableau Public Tableau Public.   "
  }
  
]

